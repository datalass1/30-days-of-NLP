# 30-days-of-NLP
September 2021, 30 Days of NLP Learning and Code

## Key Notes
### Feature Engineering:
"Word2Vec uses a trick you may have seen elsewhere in machine learning. It trains a simple neural network with a single hidden layer to perform a certain task, but then does not actually use that neural network for the task it is trained on! Instead, the goal is actually just to learn the weights of the hidden layer. We’ll see that these weights are actually the “word vectors” that we’re trying to learn." http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

### Resources:
- [1] Practical Natural Language Processing by Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, and Harshit Surana
- [2] [On word embeddings - Part 1 by SEBASTIAN RUDER](https://ruder.io/word-embeddings-1/)
- [3] [word2vec Parameter Learning Explained by Xin Rong](https://arxiv.org/pdf/1411.2738.pdf)
- [4] [Efficient Estimation of Word Representations in Vector Space by Mikolov et al](https://arxiv.org/pdf/1301.3781.pdf): The Word2vec model is in many ways the dawn of modern-day NLP.
- [5] Chris McCormick, [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
- [6] Dan Jurafsky and James H. Martin, [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/) - recommended for it's short but concise overview of different neural network methods for NLP.
