{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d2fef88",
   "metadata": {},
   "source": [
    "### fastText\n",
    "\n",
    "A method to deal with out of vocabulary (OOV). And it is extremely fast at learning on large corpora. \n",
    "\n",
    "FastText is based on the idea of enriching word embeddings with subword-level information. Thus, the embedding representation for each word is represented as a sum of the representations of individual **character** n-grams.\n",
    "\n",
    "FastText is a text classification model. \n",
    "\n",
    "We’ll work with the DBpedia dataset containined 14 classes of 560,000 training examples and 70,000 texting examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a61455e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import wget\n",
    "import tarfile\n",
    "from fasttext import train_supervised \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52017247",
   "metadata": {},
   "source": [
    "### Download & load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2ba2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:(560000, 3) Test:(70000, 3)\n"
     ]
    }
   ],
   "source": [
    "data_path = 'DATAPATH'\n",
    "train_file = data_path + '/dbpedia_csv/train.csv'\n",
    "test_file = data_path + '/dbpedia_csv/test.csv'\n",
    "if not os.path.exists(train_file):\n",
    "    !wget -P DATAPATH https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz\n",
    "    !tar -xvf DATAPATH/dbpedia_csv.tar.gz -C DATAPATH\n",
    "    \n",
    "df = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
    "df_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
    "print(\"Train:{} Test:{}\".format(df.shape,df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f41ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34566</th>\n",
       "      <td>1</td>\n",
       "      <td>Sterling Piano Company</td>\n",
       "      <td>The Sterling Piano Company was a piano manufa...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223092</th>\n",
       "      <td>6</td>\n",
       "      <td>NYC S-Motor</td>\n",
       "      <td>S-Motor was the class designation given by th...</td>\n",
       "      <td>MeanOfTransportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110270</th>\n",
       "      <td>3</td>\n",
       "      <td>Axel Zwingenberger</td>\n",
       "      <td>Axel Zwingenberger (born May 7 1955 Hamburg G...</td>\n",
       "      <td>Artist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365013</th>\n",
       "      <td>10</td>\n",
       "      <td>Sceptrophasma hispidulum</td>\n",
       "      <td>Sceptrophasma hispidulum commonly known as th...</td>\n",
       "      <td>Animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311625</th>\n",
       "      <td>8</td>\n",
       "      <td>Nucet River (Chiojdeanca)</td>\n",
       "      <td>The Nucet River is a tributary of the Chiojde...</td>\n",
       "      <td>NaturalPlace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        class                       name  \\\n",
       "34566       1     Sterling Piano Company   \n",
       "223092      6                NYC S-Motor   \n",
       "110270      3         Axel Zwingenberger   \n",
       "365013     10   Sceptrophasma hispidulum   \n",
       "311625      8  Nucet River (Chiojdeanca)   \n",
       "\n",
       "                                              description  \\\n",
       "34566    The Sterling Piano Company was a piano manufa...   \n",
       "223092   S-Motor was the class designation given by th...   \n",
       "110270   Axel Zwingenberger (born May 7 1955 Hamburg G...   \n",
       "365013   Sceptrophasma hispidulum commonly known as th...   \n",
       "311625   The Nucet River is a tributary of the Chiojde...   \n",
       "\n",
       "                  class_name  \n",
       "34566                Company  \n",
       "223092  MeanOfTransportation  \n",
       "110270                Artist  \n",
       "365013                Animal  \n",
       "311625          NaturalPlace  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping from class number to class name\n",
    "class_dict={\n",
    "            1:'Company',\n",
    "            2:'EducationalInstitution',\n",
    "            3:'Artist',\n",
    "            4:'Athlete',\n",
    "            5:'OfficeHolder',\n",
    "            6:'MeanOfTransportation',\n",
    "            7:'Building',\n",
    "            8:'NaturalPlace',\n",
    "            9:'Village',\n",
    "            10:'Animal',\n",
    "            11:'Plant',\n",
    "            12:'Album',\n",
    "            13:'Film',\n",
    "            14:'WrittenWork'}\n",
    "# Mapping the classes\n",
    "df['class_name'] = df['class'].map(class_dict)\n",
    "df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c60a8",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf6933dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_it(text,normalize=True):\n",
    "    # Replacing possible issues with data. We can add or reduce the replacemtent in this chain\n",
    "    s = str(text).replace(',',' ').replace('\"','').replace('\\'',' \\' ').replace('.',' . ').replace('(',' ( ').\\\n",
    "            replace(')',' ) ').replace('!',' ! ').replace('?',' ? ').replace(':',' ').replace(';',' ').lower()\n",
    "    # normalizing / encoding the text\n",
    "    if normalize:\n",
    "        s = s.normalize('NFKD').str.encode('ascii','ignore').str.decode('utf-8')\n",
    "    return s\n",
    "\n",
    "# Now lets define a small function where we can use above cleaning on datasets\n",
    "def clean_df(data, cleanit= False, shuffleit=False, encodeit=False, label_prefix='__class__'):\n",
    "    # Defining the new data\n",
    "    df = data[['name','description']].copy(deep=True)\n",
    "    df['class'] = label_prefix + data['class'].astype(str) + ' '\n",
    "    # cleaning it\n",
    "    if cleanit:\n",
    "        df['name'] = df['name'].apply(lambda x: clean_it(x,encodeit))\n",
    "        df['description'] = df['description'].apply(lambda x: clean_it(x,encodeit))\n",
    "    # shuffling it\n",
    "    if shuffleit:\n",
    "        df.sample(frac=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Write files to disk as fastText classifier API reads files from disk.\n",
    "train_file = data_path + '/dbpedia_train.csv'\n",
    "test_file = data_path + '/dbpedia_test.csv'\n",
    "columns = ['class','name','description']\n",
    "if not os.path.exists(train_file):\n",
    "    # Transform the datasets using the above clean functions\n",
    "    df_train_cleaned = clean_df(df, True, True)\n",
    "    df_test_cleaned = clean_df(df_test, True, True)\n",
    "    df_train_cleaned.to_csv(train_file, header=None, index=False, columns=columns )\n",
    "    df_test_cleaned.to_csv(test_file, header=None, index=False, columns=columns )\n",
    "else:\n",
    "    df_train_cleaned = pd.read_csv(train_file, names=columns)\n",
    "    df_test_cleaned = pd.read_csv(test_file, names=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5a4f3",
   "metadata": {},
   "source": [
    "All the labels start by the `__label__` prefix (in this example we use `__class__`), which is how fastText recognize what is a label or what is a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0554a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34566</th>\n",
       "      <td>__class__1</td>\n",
       "      <td>sterling piano company</td>\n",
       "      <td>the sterling piano company was a piano manufa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223092</th>\n",
       "      <td>__class__6</td>\n",
       "      <td>nyc s-motor</td>\n",
       "      <td>s-motor was the class designation given by th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110270</th>\n",
       "      <td>__class__3</td>\n",
       "      <td>axel zwingenberger</td>\n",
       "      <td>axel zwingenberger  ( born may 7 1955 hamburg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365013</th>\n",
       "      <td>__class__10</td>\n",
       "      <td>sceptrophasma hispidulum</td>\n",
       "      <td>sceptrophasma hispidulum commonly known as th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311625</th>\n",
       "      <td>__class__8</td>\n",
       "      <td>nucet river  ( chiojdeanca )</td>\n",
       "      <td>the nucet river is a tributary of the chiojde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               class                           name  \\\n",
       "34566    __class__1          sterling piano company   \n",
       "223092   __class__6                     nyc s-motor   \n",
       "110270   __class__3              axel zwingenberger   \n",
       "365013  __class__10        sceptrophasma hispidulum   \n",
       "311625   __class__8   nucet river  ( chiojdeanca )    \n",
       "\n",
       "                                              description  \n",
       "34566    the sterling piano company was a piano manufa...  \n",
       "223092   s-motor was the class designation given by th...  \n",
       "110270   axel zwingenberger  ( born may 7 1955 hamburg...  \n",
       "365013   sceptrophasma hispidulum commonly known as th...  \n",
       "311625   the nucet river is a tributary of the chiojde...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cleaned.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288169d",
   "metadata": {},
   "source": [
    "### Model training\n",
    "Notice that we gave the classifier raw text and not the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "518351e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 31M words\n",
      "Number of words:  1116962\n",
      "Number of labels: 14\n",
      "Progress: 100.0% words/sec/thread:  598277 lr:  0.000078 avg.loss:  0.003162 ETA:   0h 0m 0s33m59s avg.loss:  0.056193 ETA:   0h34m17s 580394 lr:  0.985471 avg.loss:  0.056543 ETA:   0h33m45s 0.985384 avg.loss:  0.056265 ETA:   0h33m46s 0.984468 avg.loss:  0.053297 ETA:   0h33m46s lr:  0.984209 avg.loss:  0.052462 ETA:   0h33m44s avg.loss:  0.051262 ETA:   0h33m45s  1.7% words/sec/thread:  580071 lr:  0.983177 avg.loss:  0.049215 ETA:   0h33m41s  0h33m38s 0.982795 avg.loss:  0.047964 ETA:   0h33m36s  0h33m33s 0.980356 avg.loss:  0.043070 ETA:   0h33m32s% words/sec/thread:  580492 lr:  0.979637 avg.loss:  0.041645 ETA:   0h33m32s  2.1% words/sec/thread:  580861 lr:  0.979280 avg.loss:  0.041615 ETA:   0h33m30s% words/sec/thread:  581498 lr:  0.978668 avg.loss:  0.040631 ETA:   0h33m27s 0.978468 avg.loss:  0.040468 ETA:   0h33m26s avg.loss:  0.040320 ETA:   0h33m26s% words/sec/thread:  580965 lr:  0.978146 avg.loss:  0.040000 ETA:   0h33m28s ETA:   0h33m26s26s 0.977595 avg.loss:  0.039108 ETA:   0h33m26s 0.977347 avg.loss:  0.039285 ETA:   0h33m25s avg.loss:  0.039216 ETA:   0h33m24s 0.039168 ETA:   0h33m23s% words/sec/thread:  581572 lr:  0.976896 avg.loss:  0.038882 ETA:   0h33m23s 0.976384 avg.loss:  0.038318 ETA:   0h33m20s 0.038187 ETA:   0h33m20s 0.037743 ETA:   0h33m19s 582332 lr:  0.975587 avg.loss:  0.037241 ETA:   0h33m18s lr:  0.975228 avg.loss:  0.037437 ETA:   0h33m16s ETA:   0h33m15s avg.loss:  0.036998 ETA:   0h33m15s 0.974545 avg.loss:  0.036888 ETA:   0h33m15s avg.loss:  0.036196 ETA:   0h33m11s 0.973157 avg.loss:  0.036705 ETA:   0h33m 8s words/sec/thread:  583853 lr:  0.972664 avg.loss:  0.036753 ETA:   0h33m 7s 0.971757 avg.loss:  0.036310 ETA:   0h33m 3s 584280 lr:  0.971510 avg.loss:  0.036048 ETA:   0h33m 3s  2.9% words/sec/thread:  584328 lr:  0.970620 avg.loss:  0.035571 ETA:   0h33m 1s avg.loss:  0.016543 ETA:   0h30m26s 0.911550 avg.loss:  0.016539 ETA:   0h30m25s% words/sec/thread:  596045 lr:  0.872912 avg.loss:  0.012586 ETA:   0h29m 6s 0.872565 avg.loss:  0.012546 ETA:   0h29m 6s 0.871964 avg.loss:  0.012519 ETA:   0h29m 5s 12.8% words/sec/thread:  596013 lr:  0.871862 avg.loss:  0.012509 ETA:   0h29m 4s 0.870517 avg.loss:  0.012419 ETA:   0h29m 2s 0.869773 avg.loss:  0.012358 ETA:   0h29m 0s 0.012293 ETA:   0h28m59s ETA:   0h28m58s 13.2% words/sec/thread:  595737 lr:  0.868351 avg.loss:  0.012258 ETA:   0h28m58s 0.012215 ETA:   0h28m57s avg.loss:  0.012196 ETA:   0h28m57s avg.loss:  0.012176 ETA:   0h28m56s28m27s 590809 lr:  0.722201 avg.loss:  0.007316 ETA:   0h24m18sh24m16s 590782 lr:  0.720768 avg.loss:  0.007283 ETA:   0h24m15s 590804 lr:  0.720657 avg.loss:  0.007285 ETA:   0h24m14s 0.007336 ETA:   0h24m14s 0.007323 ETA:   0h24m12s 0.718687 avg.loss:  0.007312 ETA:   0h24m11s 0.716616 avg.loss:  0.007323 ETA:   0h24m 6s lr:  0.716039 avg.loss:  0.007308 ETA:   0h24m 5s 0.715439 avg.loss:  0.007288 ETA:   0h24m 4s 0.715084 avg.loss:  0.007277 ETA:   0h24m 3s avg.loss:  0.007280 ETA:   0h24m 3s 590767 lr:  0.714737 avg.loss:  0.007272 ETA:   0h24m 3s  0h24m 1s 0.713369 avg.loss:  0.007232 ETA:   0h24m 0s 590593 lr:  0.710230 avg.loss:  0.007162 ETA:   0h23m54s 0.701556 avg.loss:  0.007013 ETA:   0h23m36s% words/sec/thread:  590562 lr:  0.697610 avg.loss:  0.006944 ETA:   0h23m28s 0.668859 avg.loss:  0.006481 ETA:   0h22m31s lr:  0.668520 avg.loss:  0.006476 ETA:   0h22m31s 0.006470 ETA:   0h22m30s  0h21m23s 0.634490 avg.loss:  0.006045 ETA:   0h21m21s 40.5% words/sec/thread:  591661 lr:  0.594920 avg.loss:  0.005551 ETA:   0h19m59s 0.554559 avg.loss:  0.005125 ETA:   0h18m38s 591453 lr:  0.554469 avg.loss:  0.005124 ETA:   0h18m38s 0.005123 ETA:   0h18m37s words/sec/thread:  591872 lr:  0.501148 avg.loss:  0.004650 ETA:   0h16m49s  0h13m28s 0.400315 avg.loss:  0.003989 ETA:   0h13m26s 0.353890 avg.loss:  0.003753 ETA:   0h11m51s 0.353781 avg.loss:  0.003752 ETA:   0h11m51s 0.296143 avg.loss:  0.003506 ETA:   0h 9m55s 593212 lr:  0.292957 avg.loss:  0.003496 ETA:   0h 9m49s avg.loss:  0.002972 ETA:   0h 3m28s 0.103985 avg.loss:  0.002971 ETA:   0h 3m28s words/sec/thread:  596231 lr:  0.103873 avg.loss:  0.002971 ETA:   0h 3m27s39s 0.018623 avg.loss:  0.002919 ETA:   0h 0m37s 98.2% words/sec/thread:  598040 lr:  0.018357 avg.loss:  0.002919 ETA:   0h 0m36s 98.2% words/sec/thread:  598059 lr:  0.017972 avg.loss:  0.002919 ETA:   0h 0m35s lr:  0.017619 avg.loss:  0.002918 ETA:   0h 0m35s 98.2% words/sec/thread:  598053 lr:  0.017528 avg.loss:  0.002918 ETA:   0h 0m34s 98.3% words/sec/thread:  598062 lr:  0.017109 avg.loss:  0.002918 ETA:   0h 0m34s 98.3% words/sec/thread:  598068 lr:  0.016998 avg.loss:  0.002920 ETA:   0h 0m33s avg.loss:  0.002924 ETA:   0h 0m32s 598117 lr:  0.015453 avg.loss:  0.002924 ETA:   0h 0m30s lr:  0.014720 avg.loss:  0.002933 ETA:   0h 0m29s 0.013691 avg.loss:  0.002935 ETA:   0h 0m27s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 6min 16s, sys: 55.9 s, total: 1h 7min 12s\n",
      "Wall time: 33min 37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress: 100.0% words/sec/thread:  598276 lr:  0.000029 avg.loss:  0.003170 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread:  598263 lr: -0.000000 avg.loss:  0.003172 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread:  598263 lr:  0.000000 avg.loss:  0.003172 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = train_supervised(input=train_file, label=\"__class__\", \n",
    "                         lr=1.0, epoch=75, loss='ova', wordNgrams=2, \n",
    "                         dim=200, thread=2, verbose=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab2ece",
   "metadata": {},
   "source": [
    "Evaluate fast text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25898c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Samples: 70000     Precision@1 : 88.3500     Recall@1 : 88.3500\n",
      "Test Samples: 70000     Precision@2 : 47.0121     Recall@2 : 94.0243\n",
      "Test Samples: 70000     Precision@3 : 31.8224     Recall@3 : 95.4671\n",
      "Test Samples: 70000     Precision@4 : 24.1404     Recall@4 : 96.5614\n",
      "Test Samples: 70000     Precision@5 : 19.4271     Recall@5 : 97.1357\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,6):\n",
    "    results = model.test(test_file,k=k)\n",
    "    print(f\"Test Samples: {results[0]} \\\n",
    "    Precision@{k} : {results[1]*100:2.4f} \\\n",
    "    Recall@{k} : {results[2]*100:2.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740f761",
   "metadata": {},
   "source": [
    "The only downside is that the trained model carries the entire character n-gram embeddings dictionary with it. This results in a bulky model and can result in engineering issues. \n",
    "\n",
    "However, fastText implementation also comes with options to reduce the memory footprint of its classification models with minimal reduction in classification performance. It does this by doing vocabulary pruning and using compression algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81043d",
   "metadata": {},
   "source": [
    "### Let's try (for fun) a logistic regression model with BoW embeddings on the same dataset to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bcada65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (560000, 669349)\n",
      "Shape of testing data: (70000, 669349)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x669349 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 29 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(df_train_cleaned['description'])\n",
    "X_test = count_vect.transform(df_test_cleaned['description'])\n",
    "print(f'Shape of training data: {X_train.shape}')\n",
    "print(f'Shape of testing data: {X_test.shape}')\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1aa2a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560000,) (70000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34566      1 \n",
       "223092     6 \n",
       "110270     3 \n",
       "365013    10 \n",
       "311625     8 \n",
       "Name: class, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df_train_cleaned['class'].apply(lambda x: x.replace('__class__', ''))\n",
    "y_test = df_test_cleaned['class'].apply(lambda x: x.replace('__class__', ''))\n",
    "print(y_train.shape, y_test.shape)\n",
    "y_train.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c3b46c",
   "metadata": {},
   "source": [
    "There is 560000 documents in the corpus. \n",
    "\n",
    "There is 669349 individual words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f034cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:  ['0343', '034366', '0345340744', '0345494016', '0346', '034937', '035', '0352', '03521', '03545']\n",
      "Feature names:  ['anogen', 'anogramma', 'anoguttata', 'anohana', 'anoia', 'anoiapithecus', 'anoiksi', 'anoint', 'anointed', 'anois']\n",
      "Feature names:  ['２３時の音楽', '３ｄ', '４０７', '４ｘ１０月', '５人', '７０１号怨み節', '８時間の恐怖', 'ｇ線上の猫', 'ｇｅ', 'ｎｅｏ']\n"
     ]
    }
   ],
   "source": [
    "print('Feature names: ', count_vect.get_feature_names()[310:320])\n",
    "print('Feature names: ', count_vect.get_feature_names()[40030:40040])\n",
    "print('Feature names: ', count_vect.get_feature_names()[-20:-10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff0b3f",
   "metadata": {},
   "source": [
    "Looks like I should remove stopwords, special characters, numbers and lemmatise words. \n",
    "\n",
    "Does fastText do this within the algorithm when creating the embeddings?\n",
    "\n",
    "Anyway, lets see what happens without more text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62f4427a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rachel/miniconda3/envs/nlp-genism/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2a50fd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9805142857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1        0.95      0.95      0.95      5000\n",
      "         10        0.99      0.99      0.99      5000\n",
      "         11        0.99      0.99      0.99      5000\n",
      "         12        0.99      0.99      0.99      5000\n",
      "         13        0.98      0.99      0.98      5000\n",
      "         14        0.98      0.96      0.97      5000\n",
      "          2        0.98      0.97      0.98      5000\n",
      "          3        0.96      0.97      0.97      5000\n",
      "          4        0.99      0.99      0.99      5000\n",
      "          5        0.98      0.98      0.98      5000\n",
      "          6        0.98      0.99      0.99      5000\n",
      "          7        0.96      0.97      0.97      5000\n",
      "          8        0.99      0.99      0.99      5000\n",
      "          9        0.99      0.99      0.99      5000\n",
      "\n",
      "    accuracy                           0.98     70000\n",
      "   macro avg       0.98      0.98      0.98     70000\n",
      "weighted avg       0.98      0.98      0.98     70000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = clf.predict(X_test) # make class predictions for X_test_dtm\n",
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]  # class probabilities\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_class)}')\n",
    "print(classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a21081",
   "metadata": {},
   "source": [
    "What!? BoW with logistic regression did really well! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becec22c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
