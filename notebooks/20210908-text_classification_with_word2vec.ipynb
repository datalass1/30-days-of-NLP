{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d6b589",
   "metadata": {},
   "source": [
    "# Word2Vec for Text Classification\n",
    "We will use the sentiment labelled sentences dataset from UCI repository http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    "The dataset consists of 1500 positive, and 1500 negative sentiment sentences from Amazon, Yelp, IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8753f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rachel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rachel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#basic imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import wget\n",
    "import gzip\n",
    "import shutil\n",
    "from time import time\n",
    "\n",
    "#pre-processing imports\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "#imports related to modeling\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd810572",
   "metadata": {},
   "source": [
    "Download & load the pre-trained embedding model, we will use the Google News vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2fa234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 1min 10s, total: 1min 13s\n",
      "Wall time: 1min 13s\n",
      "done loading Word2Vec\n"
     ]
    }
   ],
   "source": [
    "path_to_model = 'DATAPATH/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "if not os.path.exists(path_to_model):\n",
    "    !mkdir DATAPATH\n",
    "    !wget -P DATAPATH https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "    !gunzip DATAPATH/GoogleNews-vectors-negative300.bin.gz\n",
    "\n",
    "#Load W2V model. This will take some time. \n",
    "%time w2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "print('done loading Word2Vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d5f35",
   "metadata": {},
   "source": [
    "\"Word2Vec... model that embeds words in a lower-dimensional vector space using a shallow neural network. The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings. For example, strong and powerful would be close together and strong and Paris would be relatively far.\" https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "\n",
    "This site provides a simply way to get the model (for future note):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cd86698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# import gensim.downloader as api\n",
    "# wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa68c9",
   "metadata": {},
   "source": [
    "Download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a705a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "training_data_path = f\"Data/sentiment_sentences.txt\"\n",
    "if not os.path.exists(training_data_path):\n",
    "    github_prefix = 'https://github.com/practical-nlp/practical-nlp-code/blob/master/Ch4/Data/sentiment%20labelled%20sentences'\n",
    "    !mkdir Data\n",
    "    !wget -P Data {github_prefix}/amazon_cells_labelled.txt?raw=True\n",
    "    !wget -P Data {github_prefix}/imdb_labelled.txt?raw=True\n",
    "    !wget -P Data {github_prefix}/yelp_labelled.txt?raw=True\n",
    "    file = open(fil, 'w')\n",
    "    file.close()\n",
    "    filenames = ['amazon_cells_labelled.txt?raw=True', 'imdb_labelled.txt?raw=True', 'yelp_labelled.txt?raw=True']\n",
    "    with open(training_data_path, 'w') as outfile:\n",
    "        for fname in filenames:\n",
    "            with open(f'Data/{fname}') as infile:\n",
    "                outfile.write(infile.read())\n",
    "    print(\"File created\")\n",
    "else:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254f652",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea6d764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
       "  0),\n",
       " ('Good case, Excellent value.', 1),\n",
       " ('Great for the jawbone.', 1),\n",
       " ('Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!',\n",
       "  0),\n",
       " ('The mic is great.', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the file consists of tab separated sentences and categories {1: positive, 0:negative}\n",
    "texts = []\n",
    "cats = []\n",
    "fh = open(training_data_path)\n",
    "for line in fh:\n",
    "    text, sentiment = line.split(\"\\t\")\n",
    "    texts.append(text)\n",
    "    cats.append(int(sentiment[:1]))\n",
    "#Inspect the dataset\n",
    "print(len(cats), len(texts))\n",
    "list(zip(texts[:5], cats[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fa377",
   "metadata": {},
   "source": [
    "### preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44ee8557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length training data: 3000\n",
      "Length labelled data: 3000\n",
      "Original text: Good case, Excellent value.\n",
      "Pre-processed text: ['good', 'case', 'excellent', 'value']\n",
      "Is text positive (1) or negative (0)?: 1\n"
     ]
    }
   ],
   "source": [
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stops_digits(tokens):\n",
    "        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n",
    "        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit()\n",
    "               and token not in punctuation]\n",
    "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
    "\n",
    "texts_processed = preprocess_corpus(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2593e357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length training data: 3000\n",
      "Length labelled data: 3000\n",
      "\n",
      "Original text: So there is no way for me to plug it in here in the US unless I go by a converter.\n",
      "Pre-processed text: ['way', 'plug', 'us', 'unless', 'go', 'converter']\n",
      "Labelled data positive (1) or negative (0)?: 0\n",
      "\n",
      "Original text: Good case, Excellent value.\n",
      "Pre-processed text: ['good', 'case', 'excellent', 'value']\n",
      "Labelled data positive (1) or negative (0)?: 1\n",
      "\n",
      "Original text: Great for the jawbone.\n",
      "Pre-processed text: ['great', 'jawbone']\n",
      "Labelled data positive (1) or negative (0)?: 1\n",
      "\n",
      "Original text: Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\n",
      "Pre-processed text: ['tied', 'charger', 'conversations', 'lasting', 'minutes.major', 'problems']\n",
      "Labelled data positive (1) or negative (0)?: 0\n",
      "\n",
      "Original text: The mic is great.\n",
      "Pre-processed text: ['mic', 'great']\n",
      "Labelled data positive (1) or negative (0)?: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Length training data: {len(cats)}\\nLength labelled data: {len(texts_processed)}\\n')\n",
    "for i in range(5):\n",
    "    print(f'Original text: {texts[i]}\\nPre-processed text: {texts_processed[i]}')\n",
    "    print(f'Labelled data positive (1) or negative (0)?: {cats[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4daaedf",
   "metadata": {},
   "source": [
    "### Data Engineering/Text Representation\n",
    "Creating a feature vector by averaging all embeddings for all sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c3572",
   "metadata": {},
   "source": [
    "First let's have a play about with the word2vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98b3a11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('French', 0.7000749707221985),\n",
       " ('extradites_Noriega', 0.6946742534637451),\n",
       " ('Belgium', 0.6933181285858154),\n",
       " ('Villebon_Sur_Yvette', 0.6776413321495056),\n",
       " ('PARIS_AFX_Gaz_de', 0.662800133228302),\n",
       " ('called_Xynthia_blew', 0.6588140726089478),\n",
       " ('Brive_la', 0.644013524055481),\n",
       " ('COLVILLE_SUR_MER', 0.6336530447006226),\n",
       " ('Paris', 0.6334909200668335),\n",
       " ('Germany', 0.6270756125450134)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('France')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f85564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, w2v_model.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d0523e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('puppy', 0.8089798092842102), ('dogs', 0.8045638203620911), ('cats', 0.7861028909683228), ('beagle', 0.7763327360153198), ('pup', 0.7641833424568176), ('pooch', 0.7634377479553223), ('pit_bull', 0.7533083558082581), ('kitten', 0.7526556849479675), ('pet', 0.7490030527114868), ('golden_retriever', 0.7472557425498962)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.most_similar(positive=['dog', 'cat'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d08c6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1597fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05444336 -0.01184082 -0.15625     0.00193024 -0.06982422]\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "good_vec = w2v_model.get_vector('good')\n",
    "print(good_vec[-5:])  # look at the last 5 vector in this embedding for 'good'\n",
    "print(good_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a053fb4",
   "metadata": {},
   "source": [
    "Note that the shape of the vector is 300. That is for ALL vectors in this word2vec model.\n",
    "\n",
    "Use this pre-learned embedding to represent features by averaging the embeddings for individual words in preprocessed text. The code snippet below shows a simple function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6975efcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embedding_feats(list_of_lists):\n",
    "    DIMENSION = 300\n",
    "    zero_vector = np.zeros(DIMENSION)\n",
    "    feats = []\n",
    "    for tokens in list_of_lists:\n",
    "        feat_for_this =  np.zeros(DIMENSION)\n",
    "        count_for_this = 0 + 1e-5 # to avoid divide-by-zero \n",
    "        for token in tokens:\n",
    "            if token in w2v_model:  # Use embeddings only for the words that are present in the dictionary\n",
    "                feat_for_this += w2v_model[token] # get the 300-dim vector for the token\n",
    "                count_for_this +=1\n",
    "        if(count_for_this!=0):\n",
    "            feats.append(feat_for_this/count_for_this) \n",
    "        else:\n",
    "            print(token)  # by printing token we can see which embeddings are not represented in word2vec. None! :)\n",
    "            feats.append(zero_vector)\n",
    "    return feats\n",
    "\n",
    "train_vectors = embedding_feats(texts_processed)\n",
    "len(train_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff901a",
   "metadata": {},
   "source": [
    "If we’re working on a custom domain whose vocabulary is remarkably different from that of the pre-trained news embeddings we used here, it would make sense to train our own embeddings to extract features.\n",
    "\n",
    "Luckily the text was all represented by this word2vec model!\n",
    "\n",
    "### Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58bd53e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8266666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       392\n",
      "           1       0.81      0.83      0.82       358\n",
      "\n",
      "    accuracy                           0.83       750\n",
      "   macro avg       0.83      0.83      0.83       750\n",
      "weighted avg       0.83      0.83      0.83       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Take any classifier (LogisticRegression here, and train/test it like before.\n",
    "classifier = LogisticRegression(random_state=42)\n",
    "train_data, test_data, train_cats, test_cats = train_test_split(train_vectors, cats)\n",
    "classifier.fit(train_data, train_cats)\n",
    "print(\"Accuracy: \", classifier.score(test_data, test_cats))\n",
    "preds = classifier.predict(test_data)\n",
    "print(classification_report(test_cats, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ebbea",
   "metadata": {},
   "source": [
    "### Disadvantages of this word2vec model\n",
    "\n",
    "If a word in our dataset was not present in the pre-trained model’s vocabulary, how will we get a representation for this word? This problem is popularly known as out of vocabulary (OOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b8bb3d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Flabbergast' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5276/3004738850.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Flabbergast'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/nlp-genism/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-genism/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-genism/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'Flabbergast' not present\""
     ]
    }
   ],
   "source": [
    "w2v_model.most_similar('Flabbergast')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd562f6",
   "metadata": {},
   "source": [
    "One solution is called fast-text: \n",
    "- This approach can handle words that did not appear in training data (OOV).\n",
    "- The implementation facilitates extremely fast learning on even very large corpora.\n",
    "\n",
    "fastText is a general-purpose library to learn the embeddings, it also supports off-the-shelf text classification by providing end-to-end classifier training and testing; i.e., we don’t have to handle feature extraction separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0c0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
