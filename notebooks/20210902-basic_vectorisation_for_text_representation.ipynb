{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf305b8",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "These days, one-hot encoding scheme is seldom used.\n",
    "\n",
    "### Bag of words (BoW)\n",
    "A classical text representation technique that has been used commonly in NLP, especially in text classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6002bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:  ['bites', 'dog', 'eats', 'food', 'man', 'meat']\n",
      "Our vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
      "BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n",
      "Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"] #Same as the earlier notebook\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "print('Feature names: ', count_vect.get_feature_names())\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#See the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", \n",
    "\n",
    "temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae352a",
   "metadata": {},
   "source": [
    "We’ll notice that the BoW representation for a sentence like “dog and dog are friends” has a value of 2 for the dimension of the word “dog,” indicating its frequency in the text. \n",
    "\n",
    "Sometimes, we don’t care about the frequency of occurrence of words in text and we only want to represent whether a word exists in the text or not. Researchers have shown that such a representation without considering frequency is useful for sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bafb2c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(binary=True)\n",
    "bow_rep_bin = count_vect.fit_transform(processed_docs)\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607db418",
   "metadata": {},
   "source": [
    "Benefits of BoW:\n",
    "- documents having the same words will have their vector representations closer to each other in Euclidean space as compared to documents with completely different words. The distance between D1 and D2 is 0 as compared to the distance between D1 and D4, which is 2. So if two documents have similar vocabulary, they’ll be closer to each other in the vector space and vice versa.\n",
    "- Simple\n",
    "- Fixed length encoding to the length of the sentence\n",
    "\n",
    "Negatives:\n",
    "- More vocab = more sparsity\n",
    "- It does not capture the similarity between different words that mean the same thing. Say we have three documents: “I run”, “I ran”, and “I ate”. BoW vectors of all three documents will be equally apart\n",
    "- does not handle out of volab words (new words not in the build corpus)\n",
    "- word order is lost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557cb411",
   "metadata": {},
   "source": [
    "### Bag of N-Grams\n",
    "Breaking text into chunks of n contiguous words (or tokens). This can help us capture some context, which earlier approaches could not do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db5232a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:  ['bites', 'bites dog', 'bites man', 'dog', 'dog bites', 'dog bites man', 'dog eats', 'dog eats meat', 'eats', 'eats food', 'eats meat', 'food', 'man', 'man bites', 'man bites dog', 'man eats', 'man eats food', 'meat']\n",
      "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n",
      "BoW representation for 'dog bites man':  [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n",
      "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#n-gram vectorization example with count vectorizer and uni, bi, trigrams\n",
    "count_vect = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "print('Feature names: ', count_vect.get_feature_names())\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a13830",
   "metadata": {},
   "source": [
    "Here are the main pros and cons of BoN:\n",
    "\n",
    "- It captures some context and word-order information in the form of n-grams.\n",
    "- Thus, resulting vector space is able to capture some semantic similarity. Documents having the same n-grams will have their vectors closer to each other in Euclidean space as compared to documents with completely different n-grams.\n",
    "- As n increases, dimensionality (and therefore sparsity) only increases rapidly.\n",
    "- It still provides no way to address the OOV problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2dc9a",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Term frequency–inverse document frequency. It aims to quantify the importance of a given word relative to other words in the document and in the corpus. It’s a commonly used representation scheme for information-retrieval systems, for extracting relevant documents from a corpus for a given text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e517be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
      "----------\n",
      "IDF for all words in the vocabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
      "----------\n",
      "All words in the vocabulary ['bites', 'dog', 'eats', 'food', 'man', 'meat']\n",
      "----------\n",
      "TFIDF representation for all documents in our corpus\n",
      " [[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
      " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
      " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
      " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
      "----------\n",
      "Tfidf representation for 'dog and man are friends':\n",
      " [[0.         0.70710678 0.         0.         0.70710678 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "print(processed_docs)\n",
    "print(\"-\"*10)\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(processed_docs)\n",
    "\n",
    "#IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "print(\"-\"*10)\n",
    "#All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names())\n",
    "print(\"-\"*10)\n",
    "\n",
    "#TFIDF representation for all documents in our corpus \n",
    "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray()) \n",
    "print(\"-\"*10)\n",
    "\n",
    "temp = tfidf.transform([\"dog and man are friends\"])\n",
    "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d02a4",
   "metadata": {},
   "source": [
    "We can use the TF-IDF vectors to calculate similarity between two texts using a similarity measure like Euclidean distance or cosine similarity. TF-IDF is a commonly used representation in application scenarios such as information retrieval and text classification. \n",
    "\n",
    "However, despite the fact that TF-IDF is better than the vectorization methods we saw earlier in terms of capturing similarities between words, it still suffers from the curse of high dimensionality.\n",
    "\n",
    "**Even today, TF-IDF continues to be a popular representation scheme for many NLP tasks, especially the initial versions of the solution.**\n",
    "\n",
    "The main drawbacks of OHE, BoW, BoN, TF-IDF is:\n",
    "- they are discrete representations and treat language units as atomic units. Therefore not capturing the relationship between words.\n",
    "- sparse and highly-dimensional representations. \n",
    "- they can't handle out of vector (OOV) words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98c359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
