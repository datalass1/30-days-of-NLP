{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e01f48a",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "**Embedding:** \n",
    "For the set of words in a corpus, embedding is a mapping between vector space coming from distributional representation to vector space coming from distributed representation.\n",
    "\n",
    "If we’re given the word “USA,” distributionally similar words could be other countries (e.g., Canada, Germany, India, etc.) or cities in the USA.\n",
    "\n",
    "Seminal work by *Mikolov et al.* showed that their neural network–based word representation model known as “Word2vec,” based on “distributional similarity,” can capture word analogy relationships. **The Word2vec model is in many ways the dawn of modern-day NLP.**\n",
    "\n",
    "Conceptually, Word2vec takes a large corpus of text as input and “learns” to represent the words in a common vector space based on the contexts in which they appear in the corpus. Given a word w and the words appearing in its context C, how do we find the vector that best represents the meaning of the word? For every word w in corpus, we start with a vector vw initialized with random values. The Word2vec model refines the values in vw by predicting vw, given the vectors for words in the context C. It does this using a two-layer neural network.\n",
    "\n",
    "### Pre-trained work embeddings\n",
    "\n",
    "Thankfully, for many scenarios, it’s not necessary to train your own embeddings, and using pre-trained word embeddings often suffices. \n",
    "\n",
    "The most popular pre-trained embeddings are Word2vec by Google, GloVe by Stanford, and fasttext embeddings by Facebook.\n",
    "\n",
    "The following code shows loading a pre-trained word2vec embedding model and we find words that are semantically similar to the word beautiful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea778872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import gzip\n",
    "import shutil\n",
    "import warnings #This module ignores the various types of warnings generated\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import psutil #This module helps in retrieving information on running processes and system resource utilization\n",
    "from psutil import virtual_memory\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "gn_vec_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "if not os.path.exists(\"GoogleNews-vectors-negative300.bin\"):\n",
    "    if not os.path.exists(\"../Ch2/GoogleNews-vectors-negative300.bin\"):\n",
    "        #Downloading the reqired model\n",
    "        if not os.path.exists(\"../Ch2/GoogleNews-vectors-negative300.bin.gz\"):\n",
    "            if not os.path.exists(\"GoogleNews-vectors-negative300.bin.gz\"):\n",
    "                wget.download(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\")\n",
    "            gn_vec_zip_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "        else:\n",
    "            gn_vec_zip_path = \"../Ch2/GoogleNews-vectors-negative300.bin.gz\"\n",
    "        #Extracting the required model\n",
    "        with gzip.open(gn_vec_zip_path, 'rb') as f_in:\n",
    "            with open(gn_vec_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    else:\n",
    "        gn_vec_path = \"../Ch2/\" + gn_vec_path\n",
    "\n",
    "print(f\"Model at {gn_vec_path}\")\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "mem = virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c047ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainedpath = gn_vec_path\n",
    "\n",
    "#Load W2V model. This will take some time, but it is a one time effort! \n",
    "pre = process.memory_info().rss\n",
    "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
    "print('-'*10)\n",
    "\n",
    "start_time = time.time() #Start the timer\n",
    "ttl = mem.total #Toal memory available\n",
    "\n",
    "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) #load the model\n",
    "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
    "print('-'*10)\n",
    "\n",
    "print('Finished loading Word2Vec')\n",
    "print('-'*10)\n",
    "\n",
    "post = process.memory_info().rss\n",
    "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Numver of words in vocablulary: \",len(w2v_model.vocab)) #Number of words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us examine the model by knowing what the most similar words are, for a given word!\n",
    "w2v_model.most_similar('beautiful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9093c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us try with another word! \n",
    "w2v_model.most_similar('toronto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3945ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the vector representation for a word? \n",
    "w2v_model['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What if I am looking for a word that is not in this vocabulary?\n",
    "w2v_model['practicalnlp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408196f",
   "metadata": {},
   "source": [
    "Two things to note while using pre-trained models:\n",
    "- Tokens/Words are always lowercased. If a word is not in the vocabulary, the model throws an exception.\n",
    "- So, it is always a good idea to encapsulate those statements in try/except blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e518e",
   "metadata": {},
   "source": [
    "#### Getting embedding representation for full text\n",
    "With spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3cb11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# ! python -m spacy download en_core_web_md # get the spaCy model\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "mydoc = nlp(\"Canada is a large country\")\n",
    "print(mydoc.vector) #Averaged vector for the entire sentence\n",
    "#What happens when I give a sentence with strange words (and stop words), and try to get its word vector in Spacy?\n",
    "temp = nlp('practicalnlp is a newword')\n",
    "temp[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef73d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
